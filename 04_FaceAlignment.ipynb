{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face alignment using state of the art model\n",
    "\n",
    "[Papers With Code](https://paperswithcode.com/about) is awesome resource for Machine Learning information, both papers and code.\n",
    "\n",
    "For this experiment I chose [leading method (as of 22.12.2020)](https://paperswithcode.com/paper/towards-fast-accurate-and-stable-3d-dense-1) on face alignment task.\n",
    "\n",
    "Which is conveniently  avialable as [PyTorch model on GitHub](https://github.com/cleardusk/3DDFA_V2https://github.com/cleardusk/3DDFA_V2).\n",
    "\n",
    "````bash\n",
    "# that repository is prerequirement here\n",
    "git clone https://github.com/cleardusk/3DDFA_V2.git\n",
    "cd 3DDFA_V2\n",
    "./build.sh\n",
    "pip3 install -r requirements.txt\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from jetutils import SimpleTimer\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import cv2\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "sys.path.append('3DDFA_V2')\n",
    "from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "\n",
    "from TDDFA import TDDFA\n",
    "from utils.functions import draw_landmarks, cv_draw_landmark\n",
    "from utils.render import render\n",
    "from utils.depth import depth\n",
    "from utils.pncc import pncc\n",
    "from utils.pose import viz_pose\n",
    "\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "from jetutils import SimpleTimer\n",
    "from IPython.display import display\n",
    "from sidecar import Sidecar\n",
    "from torch2trt import TRTModule\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original ONNX runtime model processing time is ~45ms and face detection time ~100ms\n",
    "rendering time being 50ms per image.\n",
    "\n",
    "When converted to TensorRT model time drops to ~25ms per image. Still rendering and face detection take too much time, best frame rate being about 5/s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "        \n",
    "MODEL_PATH='models/mb1_120x120_trt.pth'\n",
    "FACE_BOX_PATH='models/facebox_trt.pth'\n",
    "MODEL_H = 120\n",
    "MODEL_W = 120\n",
    "\n",
    "cfg = yaml.load(open('3DDFA_V2/configs/mb1_120x120.yml'), Loader=yaml.SafeLoader)\n",
    "\n",
    "#patch config paths\n",
    "def patch_path(config, key):\n",
    "    config[key] = '3DDFA_V2/{}'.format(config[key])\n",
    "\n",
    "for key in ('checkpoint_fp', 'bfm_fp'):\n",
    "    patch_path(cfg, key)\n",
    "\n",
    "tddfa = TDDFA(gpu_mode=True, **cfg)\n",
    "face_boxes = FaceBoxes_ONNX()\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    import torch2trt\n",
    "    timer = SimpleTimer()\n",
    "    with timer:\n",
    "        trt_model = torch2trt.torch2trt(tddfa.model,\n",
    "                                        [torch.zeros((1, 3, MODEL_H, MODEL_W)).cuda()],\n",
    "                                        fp16_mode=True,\n",
    "                                        max_workspace_size=1<<20)\n",
    "    print('model converted to TensorRT in {} seconds'.format(timer.time))\n",
    "    torch.save(trt_model.state_dict(), MODEL_PATH)        \n",
    "    tddfa.model = trt_model\n",
    "else:\n",
    "    timer = SimpleTimer()\n",
    "    with timer:\n",
    "        trt_model = TRTModule()\n",
    "        trt_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    print('model loaded in {} seconds'.format(timer.time))\n",
    "    tddfa.model = trt_model\n",
    "\n",
    "# TODO: figure out proper convertion for face box detection model\n",
    "#if not os.path.exists(FACE_BOX_PATH):\n",
    "#    import torch2trt\n",
    "#    timer = SimpleTimer()\n",
    "#    with timer:\n",
    "#        trt_model_fb = torch2trt.torch2trt(face_boxes.net.cuda(),\n",
    "#                                          [torch.zeros((1, 3, MODEL_H, MODEL_W)).cuda()],\n",
    "#                                          fp16_mode=True,\n",
    "#                                          max_workspace_size=1<<20)\n",
    "#    print('face box converted to TensorRT in {} seconds'.format(timer.time))\n",
    "#    torch.save(trt_model_fb.state_dict(), FACE_BOX_PATH)        \n",
    "#    face_boxes.net = trt_model_fb\n",
    "#else:\n",
    "#    timer = SimpleTimer()\n",
    "#    with timer:\n",
    "#        trt_model_fb = TRTModule()\n",
    "#        trt_model_fb.load_state_dict(torch.load(FACE_BOX_PATH))\n",
    "#    print('face box loaded in {} seconds'.format(timer.time))\n",
    "#    face_boxes.net = trt_model_fb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import io\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class TDDFAProcess(traitlets.HasTraits):\n",
    "    input_frame = traitlets.Any()\n",
    "    output_frame = traitlets.Any()\n",
    "    draw_original = traitlets.Bool(default_value=False)\n",
    "    render_mode = traitlets.Unicode(default_value='mask')\n",
    "    \n",
    "    dense = traitlets.Bool(default_value=True)\n",
    "    alpha = traitlets.Integer(default_value=60)\n",
    "\n",
    "    def __init__(self, model, detector):\n",
    "\n",
    "        self._model = model\n",
    "        self._detector = detector\n",
    "        self.debug_out = None\n",
    "        self.output_frame = np.zeros((640, 480, 3), dtype=np.uint8)\n",
    "    \n",
    "    def process_frame(self, img):\n",
    "        \n",
    "        img = img.astype(np.uint8)\n",
    "        timer = SimpleTimer()\n",
    "        fpst = SimpleTimer()\n",
    "        with fpst:\n",
    "            with timer:\n",
    "                boxes = self._detector(img)\n",
    "            boxtime = timer.time\n",
    "            with timer:\n",
    "                param_lst, roi_box_lst = self._model(img, boxes)\n",
    "            regtime = timer.time\n",
    "            with timer:\n",
    "                dense = self.dense if self.render_mode == 'landmarks' else True\n",
    "                ver_lst = self._model.recon_vers(param_lst, roi_box_lst,\n",
    "                                                 dense_flag=dense)\n",
    "            recontime = timer.time\n",
    "            pose_info = ''\n",
    "            with timer:\n",
    "                if self.render_mode == 'landmarks':\n",
    "                    # landmark plotting is way too slow for real time...\n",
    "                    img = draw_landmarks(img, ver_lst, show_flag=False,\n",
    "                                         dense_flag=self.dense)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "                elif self.render_mode == 'mask':\n",
    "                    img = render(img, ver_lst, self._model.tri,\n",
    "                                 alpha=float(self.alpha/100), show_flag=False,\n",
    "                                 with_bg_flag=self.draw_original)\n",
    "\n",
    "                elif self.render_mode == 'depth':\n",
    "                    img = depth(img, ver_lst, self._model.tri, show_flag=False,\n",
    "                                with_bg_flag=self.draw_original)\n",
    "                elif self.render_mode == 'pncc':\n",
    "                    img = pncc(img, ver_lst, self._model.tri, show_flag=False,\n",
    "                              with_bg_flag=self.draw_original)\n",
    "                elif self.render_mode == 'pose':\n",
    "                    if self.draw_original:\n",
    "                        img2 = img.copy()\n",
    "                    else:\n",
    "                        img2 = np.zeros_like(img)\n",
    "                    img, pose = viz_pose(img2, param_lst, ver_lst, show_flag=False)\n",
    "                    pose_info = 'yaw   {}\\npitch {}\\nroll  {}'.format(pose[0], pose[1], pose[2])\n",
    "            rendertime = timer.time\n",
    "        fps = fpst.fps\n",
    "        \n",
    "        self.output_frame = img\n",
    "        return 'faces {}\\ndetect time {}\\nmodel time  {}\\nrecon time  {}\\nrender time {}\\nfps {}\\n\\n{}'.format(\n",
    "                len(boxes), boxtime, regtime, recontime, rendertime, fps, pose_info)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls, model, detector, img_path):\n",
    "        instance = cls(model, detector)\n",
    "        # validate all rendering modes\n",
    "        instance.draw_original = True\n",
    "        \n",
    "        for mode in tqdm(('landmarks', 'mask', 'depth', 'pncc', 'pose')):\n",
    "            data  = cv2.imread(img_path)\n",
    "            instance.render_mode = mode\n",
    "            print(instance.process_frame(data))\n",
    "            display(PIL.Image.fromarray(cv2.cvtColor(instance.output_frame, cv2.COLOR_BGR2RGB))) \n",
    "\n",
    "TDDFAProcess.validate(tddfa,face_boxes, 'jetson-inference/data/images/humans_6.jpg')\n",
    "face_model = TDDFAProcess(tddfa, face_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetutils import GstCamera, bgr8_to_jpeg\n",
    "camera = GstCamera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My standard display and control box with controls for different modes and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "image_original = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "image_processed = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "image_original.value = bgr8_to_jpeg(np.zeros((camera.height, camera.width, 3), dtype=np.uint8))\n",
    "image_processed.value = bgr8_to_jpeg(np.zeros((camera.height, camera.width, 3), dtype=np.uint8))\n",
    "\n",
    "\n",
    "debug_out = ipywidgets.Textarea(value='',\n",
    "                                disabled=True,\n",
    "                                layout=ipywidgets.Layout(width='640px', height='520px'))\n",
    "face_model.debug_out = debug_out\n",
    "\n",
    "images_out = ipywidgets.HBox([image_original, image_processed])\n",
    "\n",
    "select_outmode = ipywidgets.ToggleButtons(options=['landmarks', 'mask', 'depth', 'pncc', 'pose'],\n",
    "                                     value='mask', description='render',\n",
    "                                     style={'description_width': 'initial'})\n",
    "\n",
    "select_original = ipywidgets.ToggleButton(value=False, description='over orginal')\n",
    "select_dense = ipywidgets.ToggleButton(value=True, description='dense')\n",
    "\n",
    "alpha_slider = ipywidgets.IntSlider(value=50, min=0, max=100,\n",
    "                                         description='mask alpha %',\n",
    "                                         style={'description_width': 'initial'})\n",
    "\n",
    "control_box = ipywidgets.HBox([select_outmode,\n",
    "                               ipywidgets.Label(value=' '),\n",
    "                               select_original,\n",
    "                               select_dense,\n",
    "                               alpha_slider])\n",
    "\n",
    "all_box = ipywidgets.VBox([images_out, control_box, debug_out])\n",
    "_sidecar = Sidecar(title='output')\n",
    "with _sidecar:\n",
    "    display(all_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "ipython = IPython.get_ipython()\n",
    "traitlets.dlink((camera, 'value'), (image_original, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((face_model,'output_frame'), (image_processed, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "traitlets.dlink((select_original, 'value'), (face_model, 'draw_original'))\n",
    "traitlets.dlink((select_outmode, 'value'), (face_model, 'render_mode'))\n",
    "traitlets.dlink((select_dense, 'value'), (face_model, 'dense'))\n",
    "traitlets.dlink((alpha_slider, 'value'), (face_model, 'alpha'))\n",
    "\n",
    "# this model is too slow to be run each time camera streams new image\n",
    "# so get new image only after processing old one\n",
    "while True: \n",
    "    ipython.kernel.do_one_iteration()\n",
    "    frame = camera.read()\n",
    "    debug_out.value = face_model.process_frame(frame)\n",
    "    \n",
    "#camera.observe(process, names='value')\n",
    "#camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camera"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
