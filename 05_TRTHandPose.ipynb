{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time hand pose estimation with NVIDIA TensorRT model\n",
    "This is adaptation of human pose estimatoin with new [model for hand pose](https://github.com/NVIDIA-AI-IOT/trt_pose_hand) NVIDIA\n",
    "published just recently (17.12.2020). Get the new repository and [hand model weights](https://drive.google.com/file/d/1NCVo0FiooWccDzY7hCc5MAKaoUpts3mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import ipywidgets\n",
    "import jetson.utils\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import trt_pose.coco\n",
    "\n",
    "from jetutils import SimpleTimer\n",
    "from IPython.display import display\n",
    "from sidecar import Sidecar\n",
    "from torch2trt import TRTModule\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/hand_pose_resnet18_att_244_244_trt.pth'\n",
    "MODEL_W = 224\n",
    "MODEL_H = 224\n",
    "\n",
    "with open('trt_pose_hand/preprocess/hand_pose.json', 'r') as f:\n",
    "    hand_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(hand_pose)\n",
    "\n",
    "# colors for links starting from named keypoint (BGR)\n",
    "COLOR_MAP = {\n",
    "    'palm': (255,255,255),\n",
    "    'thumb': (180,180,255),\n",
    "    'index': (180,255,180),\n",
    "    'middle': (255,180,180),\n",
    "    'ring': (255,180,255),\n",
    "    'baby': (180,180,180),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first time only we need to convert the downloaded model weights into TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded in 29.26948960100708 seconds\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    import trt_pose.models\n",
    "    import torch2trt\n",
    "    totals = SimpleTimer()\n",
    "    timer = SimpleTimer()\n",
    "    with totals:\n",
    "        MODEL_WEIGHTS = 'models/hand_pose_resnet18_att_244_244.pth'\n",
    "        num_parts = len(hand_pose['keypoints'])\n",
    "        num_links = len(hand_pose['skeleton'])\n",
    "        with timer:\n",
    "            model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "        print('model with {} parts and {} links generated in {} seconds'.format(num_parts,\n",
    "                                                                                num_links,\n",
    "                                                                                timer.time))\n",
    "        with timer:\n",
    "            model.load_state_dict(torch.load(MODEL_WEIGHTS))\n",
    "        print('model weights loaded in {} secods'.format(timer.time))\n",
    "        with timer:\n",
    "            trt_model = torch2trt.torch2trt(model,\n",
    "                                            [torch.zeros((1, 3, MODEL_H, MODEL_W)).cuda()],\n",
    "                                            fp16_mode=True,\n",
    "                                            max_workspace_size=1<<20)\n",
    "        print('model converted to TensorRT in {} seconds'.format(timer.time))\n",
    "        torch.save(trt_model.state_dict(), MODEL_PATH)\n",
    "    print('total time {} seconds'.format(totals.time))\n",
    "else:\n",
    "    timer = SimpleTimer()\n",
    "    with timer:\n",
    "        trt_model = TRTModule()\n",
    "        trt_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    print('model loaded in {} seconds'.format(timer.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import traitlets\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "from collections import namedtuple\n",
    "Keypoint = namedtuple('Keypoint', 'name x y')\n",
    "KPLink = namedtuple('KPLink', 'x0 y0 x1 y1 color')\n",
    "_timer = SimpleTimer()\n",
    "\n",
    "class HandPose(traitlets.HasTraits):\n",
    "    input_frame = traitlets.Any()\n",
    "    output_frame = traitlets.Any()\n",
    "    \n",
    "    image_width = traitlets.Integer()\n",
    "    image_height = traitlets.Integer()\n",
    "    width = traitlets.Integer(default_value=MODEL_W)\n",
    "    height = traitlets.Integer(default_value=MODEL_H)\n",
    "    \n",
    "    draw_background = traitlets.Bool(default_value=True)\n",
    "    draw_skeleton = traitlets.Bool(default_value=True)\n",
    "    draw_labels = traitlets.Bool(default_value=False)\n",
    "    \n",
    "    _X = 1\n",
    "    _Y = 0\n",
    "    \n",
    "    def __init__(self, model, topology, keypoints, \n",
    "                 *args, **kwargs):\n",
    "        super(HandPose, self).__init__(*args, **kwargs)\n",
    "        self.input_frame = np.empty((self.image_height, self.image_width, 3), dtype=np.uint8)\n",
    "        self.output_frame = np.empty((self.image_height, self.image_width, 3), dtype=np.uint8)\n",
    "        self._running = False \n",
    "        self._model = model\n",
    "        self._parse = ParseObjects(topology)\n",
    "        self._topology = topology\n",
    "        self._keypoints = keypoints\n",
    "        self._link_colors = dict()\n",
    "        for link in sorted([(int(topology[a][2]),\n",
    "                             int(topology[a][3]))\n",
    "                            for a in range(topology.shape[0])]):\n",
    "            self._link_colors[link] = COLOR_MAP[keypoints[link[0]].split('_')[0]]\n",
    "        self._mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "        self._std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "        self._device = torch.device('cuda')\n",
    "\n",
    "    def _get_hand_keypoints(self, hands, index, peaks):\n",
    "        keypoints = []\n",
    "        hand = hands[0][index]\n",
    "        C = hand.shape[0]\n",
    "        for j in range(C):\n",
    "            k = int(hand[j])\n",
    "            if k >= 0:\n",
    "                peak = peaks[0][j][k]\n",
    "                keypoints.append(Keypoint(\n",
    "                                          name=self._keypoints[j],\n",
    "                                          x=round(float(peak[HandPose._X]) * self.image_width),\n",
    "                                          y=round(float(peak[HandPose._Y]) * self.image_height)\n",
    "                                         ))\n",
    "        return keypoints\n",
    "\n",
    "    def _get_keypoint_links(self, hands, index, peaks):\n",
    "        links = []\n",
    "        hand = hands[0][index]\n",
    "        K = self._topology.shape[0]\n",
    "        for k in range(K):\n",
    "            c_a = self._topology[k][2]\n",
    "            c_b = self._topology[k][3]\n",
    "            if hand[c_a] >= 0 and hand[c_b] >= 0:\n",
    "                peak0 = peaks[0][c_a][hand[c_a]]\n",
    "                peak1 = peaks[0][c_b][hand[c_b]]\n",
    "                clink = (int(c_a), int(c_b))\n",
    "                links.append(KPLink(\n",
    "                    x0=round(float(peak0[HandPose._X]) * self.image_width),\n",
    "                    y0=round(float(peak0[HandPose._Y]) * self.image_height),\n",
    "                    x1=round(float(peak1[HandPose._X]) * self.image_width),\n",
    "                    y1=round(float(peak1[HandPose._Y]) * self.image_height),\n",
    "                    color=self._link_colors[clink] if clink in self._link_colors else (255,255,255)\n",
    "                ))\n",
    "        return links\n",
    "\n",
    "    def _draw(self, image, counts, hands, peaks):\n",
    "        dbg_str = ''\n",
    "        for i in range (int(counts[0])):\n",
    "            dbg_str = '\\n\\n'.join([dbg_str,'hand {}:'.format(i)])\n",
    "            keypoints = self._get_hand_keypoints(hands, i, peaks)\n",
    "            kplinks = self._get_keypoint_links(hands, i, peaks)\n",
    "            if self.draw_skeleton:\n",
    "                for link in kplinks:\n",
    "                    cv2.line(image, (link.x0, link.y0), (link.x1, link.y1), link.color, 2)\n",
    "            for keypoint in keypoints:\n",
    "                dbg_str = '\\n'.join([dbg_str,str(keypoint)])\n",
    "                if self.draw_labels:\n",
    "                    cv2.putText(image , '{}'.format(keypoint.name),\n",
    "                                (keypoint.x + 5 + 2, keypoint.y + 5), \n",
    "                                cv2.FONT_HERSHEY_PLAIN, 1, (150, 150, 150), 1)\n",
    "                cv2.circle(image, (keypoint.x, keypoint.y), 5, (240,240,240), 1)\n",
    "        return dbg_str\n",
    "                         \n",
    "    def run_model(self):\n",
    "        self._device = torch.device('cuda')\n",
    "        with _timer:\n",
    "            image = self.input_frame.copy()\n",
    "            if self.draw_background:\n",
    "                output_frame = self.input_frame.copy()\n",
    "            else:\n",
    "                output_frame = np.zeros((self.image_height, self.image_width, 3), dtype=np.uint8)\n",
    "\n",
    "             # first scale input into model dimensions\n",
    "            image = cv2.resize(image, (self.height, self.width))\n",
    "            image = PIL.Image.fromarray(image.astype(np.uint8)).convert('RGB')\n",
    "        \n",
    "            # transform to torch tensor on CUDA device\n",
    "            image_tensor = transforms.functional.to_tensor(image).to(self._device)\n",
    "            image_tensor.sub_(self._mean[:, None, None]).div_(self._std[:, None, None])\n",
    "            input_data = image_tensor[None, ...]\n",
    "\n",
    "            # then use the ML model\n",
    "            cmap, paf = self._model(input_data)\n",
    "            cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "            h_counts, hands, peaks = self._parse(cmap, paf)\n",
    "\n",
    "            # finally draw object skeleton over image\n",
    "            dbg_str = self._draw(output_frame, h_counts, hands, peaks)\n",
    "            self.output_frame = output_frame\n",
    "        return 'fps {}{}'.format(_timer.fps, dbg_str)\n",
    "\n",
    "    @classmethod\n",
    "    def validate(cls, model, topo, kp,img_path):\n",
    "        data  = cv2.imread(img_path)\n",
    "        instance = cls(model, topo, kp, image_height=data.shape[cls._Y], image_width=data.shape[cls._X])\n",
    "        instance.input_frame = data\n",
    "        print(instance.run_model())\n",
    "        display(PIL.Image.fromarray(cv2.cvtColor(instance.output_frame, cv2.COLOR_BGR2RGB)))\n",
    "        del instance\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to validate model and above HumanPose abstraction before actual real-time usage. Makes changes and improvements a lot of easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HandPose.validate(trt_model, topology, hand_pose['keypoints'], 'hand.png')\n",
    "print(hand_pose['keypoints'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And only then initialize camera steam so in case of errors it is not needed to clean gstreamer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetutils import GstCamera, bgr8_to_jpeg\n",
    "camera = GstCamera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = HandPose(trt_model, topology, hand_pose['keypoints'],\n",
    "                       image_height=camera.height, image_width=camera.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make our common output display sidecar again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_original = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "image_processed = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "image_original.value = bgr8_to_jpeg(np.zeros((camera.height, camera.width, 3), dtype=np.uint8))\n",
    "image_processed.value = bgr8_to_jpeg(np.zeros((camera.height, camera.width, 3), dtype=np.uint8))\n",
    "\n",
    "\n",
    "debug_out = ipywidgets.Textarea(value='',\n",
    "                                disabled=True,\n",
    "                                layout=ipywidgets.Layout(width='640px', height='520px'))\n",
    "images_out = ipywidgets.HBox([image_original, image_processed])\n",
    "\n",
    "select_background = ipywidgets.ToggleButton(value=True, description='background')\n",
    "select_skeleton = ipywidgets.ToggleButton(value=True, description='skeleton')\n",
    "select_labels = ipywidgets.ToggleButton(value=False, description='labels')\n",
    "control_box = ipywidgets.HBox([ipywidgets.Label(value='Draw '),\n",
    "                               select_background,\n",
    "                               select_skeleton,\n",
    "                               select_labels])\n",
    "\n",
    "all_box = ipywidgets.VBox([images_out, control_box, debug_out])\n",
    "_sidecar = Sidecar(title='output')\n",
    "with _sidecar:\n",
    "    display(all_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With traitlest it is so convenient to link traits of objects together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "traitlets.dlink((camera, 'value'), (image_original, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((pose_model,'output_frame'), (image_processed, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "traitlets.dlink((select_background, 'value'), (pose_model, 'draw_background'))\n",
    "traitlets.dlink((select_skeleton, 'value'), (pose_model, 'draw_skeleton'))\n",
    "traitlets.dlink((select_labels, 'value'), (pose_model, 'draw_labels'))\n",
    "\n",
    "def process(change):\n",
    "    pose_model.input_frame = change['new']\n",
    "    debug_out.value = pose_model.run_model()\n",
    "    \n",
    "camera.observe(process, names='value')\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.running = False\n",
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camera\n",
    "del trt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
